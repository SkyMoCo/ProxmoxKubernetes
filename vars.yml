###############################################################################################
#                                      -General Options-                                      # 
###############################################################################################
# These parameters are required.                                                              #
###############################################################################################

# qcow2 image to use for your Virtual Machines: 
# KDS qcow2_image: 'https://cdimage.debian.org/cdimage/openstack/current-10/debian-10-openstack-amd64.qcow2'
qcow2_image: 'https://gensho.ftp.acc.umu.se/cdimage/openstack/current-10/debian-10-openstack-amd64.qcow2'
#qcow2_image: 'https://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2'
# Location to use for storing the qcow2 image. Be sure to end the absolute path with a '/'.
qcow2_download_location: '/tmp/'

# Resource Pool for your virtual machines.
# KDS - created on vbox
k8s_resource_pool: 'Kubernetes'

# Path to the SSH Public Key on your Proxmox Server.
# Note that this key in will be loaded onto the clients, so you if
# you are running as non-root, use that key so you can ssh debian@kmaster0
k8s_ssh_key: '/etc/pve/priv/k8rsa.pub'

# VM IDs
k8s_master_id: '4010'
k8s_node1_id: '40101'
k8s_node2_id: '40102'
k8s_node3_id: '40103'
k8s_node4_id: '40104'
k8s_node5_id: '40105'

# Hostnames
k8s_master_hn: 'K8master0'
k8s_node1_hn: 'K8node1'
k8s_node2_hn: 'K8node2'
k8s_node3_hn: 'K8node3'
k8s_node4_hn: 'K8node4'
k8s_node5_hn: 'K8node5'

# Number of CPUs
k8s_master_cpu: '2'
k8s_node1_cpu: '4'
k8s_node2_cpu: '4'
k8s_node3_cpu: '4'
k8s_node4_cpu: '4'
k8s_node5_cpu: '4'


# Amount of memory expressed in megabytes
k8s_master_mem: '5120'
k8s_node1_mem: '10240'
k8s_node2_mem: '10240'
k8s_node3_mem: '10240'
k8s_node4_mem: '10240'
k8s_node5_mem: '10240'

# Disk Sizes
k8s_master_size: '50G'
k8s_node1_size: '50G'
k8s_node2_size: '50G'
k8s_node3_size: '50G'
k8s_node4_size: '50G'
k8s_node5_size: '50G'

# IP Addresses
k8s_master_ip: '172.26.0.113'
k8s_node1_ip: '172.26.0.114'
k8s_node2_ip: '172.26.0.115'
k8s_node3_ip: '172.26.0.116'
k8s_node4_ip: '172.26.0.117'
k8s_node5_ip: '172.26.0.118'

# MAC Addresses
# 
k8s_master_mac: '02:a6:42:7e:53:2c'
k8s_node1_mac:  '02:5f:0a:09:aa:55'
k8s_node2_mac:  '02:db:e8:af:bb:89'
k8s_node3_mac:  '02:b6:80:d0:60:c6'
k8s_node4_mac:  '02:5b:09:8c:1c:92'
k8s_node5_mac:  '02:bf:05:38:37:6e'


# Subnet Sizes
k8s_master_sn: '/23'
k8s_node1_sn: '/23'
k8s_node2_sn: '/23'
k8s_node3_sn: '/23'
k8s_node4_sn: '/23'
k8s_node5_sn: '/23'

# Network Gateway
k8s_master_gw: '172.26.0.1'
k8s_node1_gw: '172.26.0.1'
k8s_node2_gw: '172.26.0.1'
k8s_node3_gw: '172.26.0.1'
k8s_node4_gw: '172.26.0.1'
k8s_node5_gw: '172.26.0.1'

# DNS Servers
k8s_master_ns: '172.26.0.200'
k8s_node1_ns: '172.26.0.200'
k8s_node2_ns: '172.26.0.200'
k8s_node3_ns: '172.26.0.200'
k8s_node4_ns: '172.26.0.200'
k8s_node5_ns: '172.26.0.200'

# Search Domains
k8s_master_sd: 'int.skymoco.dev'
k8s_node1_sd: 'int.skymoco.dev'
k8s_node2_sd: 'int.skymoco.dev'
k8s_node3_sd: 'int.skymoco.dev'
k8s_node4_sd: 'int.skymoco.dev'
k8s_node5_sd: 'int.skymoco.dev'

# Network Bridges
k8s_master_bridge: 'vmbr0'
k8s_node1_bridge: 'vmbr0'
k8s_node2_bridge: 'vmbr0'
k8s_node3_bridge: 'vmbr0'
k8s_node4_bridge: 'vmbr0'
k8s_node5_bridge: 'vmbr0'

# Storage volumes
k8s_master_stg: 'storage'
k8s_node1_stg: 'storage'
k8s_node2_stg: 'storage'
k8s_node3_stg: 'storage'
k8s_node4_stg: 'storage'
k8s_node5_stg: 'storage'

# CIDR for the Calico Pod Network - MUST be different from your Kubernetes CIDR to avoid an IP conflict. Subnet size will automatically be set to /16.
calico_cidr: '172.16.0.0'

# URL for the Calico Network Policy: 
# Can be found here: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#tabs-pod-install-2
calico_policy_url: 'https://docs.projectcalico.org/v3.8/manifests/calico.yaml'

# The URL for the Kubernetes Dashboard Manifest. Not locally hosted as it changes frequently.
# Can be found in the `Getting Started` section of this page: https://github.com/kubernetes/dashboard/blob/master/README.md
k8s_dashboard_url: 'https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml'

# Service user created for the dashboard.
k8s_user_name: 'kevins'


###############################################################################################
#                                     -Dashboard Options-                                     # 
###############################################################################################
# These parameters are only required if you intend to deploy the Kubernetes Dashboard to your #
# cluster. If you wish to bind the Dashboard service to a Load Balanced IP Address, then      #
# uncomment and provide the values below. If you wish to deploy the Dashboard like normal,    #
# and use `kubectl-proxy` or `kubectl port-forward` to expose it, leave them commented out.   #
#                                                                                             #
# If you exposed the Dashboard with MetalLB, you can navigate to: https://HOSTNAME:8443/      #
# If you are exposing the Dashboard with `kubectl proxy`, you can find instructions on how to #
# access the dashboard here:                                                                  #
# https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/               #
#                                                                                             #
# TKS has already provisioned a Service Account that you can use to authenticate against your #
# Dashboard. You can retrieve this token like so. Be sure to replace K8S_USER_NAME with the   #
#value that you set for `k8s_user_name` above.                                                #
# kubectl describe -n kube-system secret K8S_USER_NAME-token | grep token: | awk '{print $2}' #
###############################################################################################

# The MetalLB Address Pool from which you want to obtain an IP address.
dashboard_load_balancer_address_pool: 'VLAN50'

# The IP address within the Address Pool to which you want to bind the service. 
dashboard_load_balancer_ip: '192.168.50.100'

# The hostname with which you want to test connectivity to the IP Address after the deployment has completed.
dashboard_hostname: 'amalthea.sol.milkyway'


###############################################################################################
#                                        -VLAN Options-                                       # 
###############################################################################################
# These parameters are only required if your network is configured with VLANs and you wish to #
# specify which VLAN each of your Kubernetes nodes is allocated to. Leave these variables     #
# blank if you do not intend to use VLANs for your cluster.                                   #
###############################################################################################

# VLAN Tags
k8s_master_vlan: ''
k8s_node1_vlan: ''
k8s_node2_vlan: ''
k8s_node3_vlan: ''


###############################################################################################
#                                    -Unattended Upgrades-                                    # 
###############################################################################################
# These parameters are only required if you want to enable Unattended Upgrades on the         #
# operating systems that comprise your Kubernetes master and worker nodes. None of these      #
# variables are required to enable the feature. However, you may wish to enable email         #
# notifications via SMTP or tweak the way in which Unattended Upgrades are applied to your    #
# system. To do so, simply uncomment the relevant configuration setting.                      #
#                                                                                             #
# NOTE: If you enable any of the following variables, then you must enable the other ones as  #
# well or SMTP will not work:                                                                 #
#     - smtp_server                                                                           #
#     - smtp_port                                                                             #
#     - smtp_username                                                                         #
#     - smtp_password                                                                         #
#     - email_address                                                                         #
###############################################################################################

# SMTP Server
smtp_server: 'smtp.gmail.com'

# SMTP Port 
smtp_port: '587'

# SMTP Server username
smtp_username: 'thomaszimmerman93@gmail.com'

# SMTP Server password
smtp_password: 'PASSWORD'

# The email address to which notifications will be delivered.
email_address: 'thomaszimmerman93@gmail.com'

# You want to be naughty and use SMTP without TLS.
#smtp_use_tls: 'off'

# Only receive emails when an error occurs.
mail_only_on_error: "true"

# Only perform upgrades when a graceful OS shutdown occurs.
#update_only_on_shutdown: "true"

# Automatically remove unused kernel packages.
remove_unused_kernel_packages: "true"

# Automatically remove unused dependencies. 
remove_unused_dependencies: "true"

# Automatically perform a reboot after upgrading if required.
#automatic_reboot_if_required: "true"

# If automatic reboots are enabled, configure the time at which they occur.
#automatic_reboot_time: "02:00"

# Enable logging to syslog.
#enable_syslog_logging: "true"


###############################################################################################
#                                        -NFS Options-                                        # 
###############################################################################################
# These parameters are only required if you have an NFS server and would like to enable the   #
# optional support for dynamic provisioning of Persistent Storage volumes for your pods.      #
###############################################################################################

# NFS server, you can use your proxmox server, but you don't have to
nfs_hostname: 'rbox.int.skymoco.dev'

# NFS Mount Point
nfs_mount_point: '/spool/persistentvolumes'

# NFS Provisioner Name
nfs_provisioner: 'vboxprovisioner'

# Namespace to deploy the NFS Provisioner to.
nfs_namespace: 'nfs-provisioner'

# Default reclaim policy. https://kubernetes.io/docs/concepts/storage/storage-classes/#reclaim-policy
nfs_reclaim_policy: 'Retain'


###############################################################################################
#                                      -MetalLB Options-                                      # 
###############################################################################################
#                                                                                             #
# WARNING: MetalLB is currently supported, however dynamic provisioning is NOT currently      #
# supported despite the explanation below. This will come in a later release, however, if you #
# are interested in using this feature you will need to modify the configmap located in the   #
# MetalLB files directory yourself to match the variables you declare here.                   #
#                                                                                             #
# These parameters are only required if you want to enable on-prem load balancing with        #
# MetalLB. The MetalLB configuration file will be dynamically provisioned based on how many   #
# variable sets you declare below. This is intended to allow you to provision a single or     #
# multiple address pools depending on your personal network configuration. The configuration  #
# is dynamically generated based on how many sets of variables you list below. For example:   #
#                                                                                             #
# If you wish to deploy a single address pool, you would declare these three variables.       #
#                                                                                             #
# metallb_ap1_name: NAME                                                                      #
# metallb_ap1_protocol: PROTOCOL                                                              #
# metallb_ap1_cidr: CIDR                                                                      #
#                                                                                             #
#                                                                                             #
# However, if you wished to declare two address pools, you would include a second set of      #
# variables where the `ap1` portion of the variable name is incremeneted by one.              #
#                                                                                             #
# metallb_ap1_name: NAME                                                                      #
# metallb_ap2_name: NAME                                                                      #
# metallb_ap1_protocol: PROTOCOL                                                              #
# metallb_ap2_protocol: PROTOCOL                                                              #
# metallb_ap1_cidr: CIDR                                                                      #
# metallb_ap2_cidr: CIDR                                                                      #
###############################################################################################

# The IP Address of the router you wish to peer with.
metallb_peer_router: '172.26.0.1'

# The AS Number of the router you wish to peer with.
metallb_peer_asn: '64512'

# The names of your address pools.
metallb_ap1_name: 'VLAN10'
metallb_ap4_name: 'VLAN70'
metallb_ap2_name: 'VLAN50'
metallb_ap3_name: 'VLAN60'

# The protocols of your address pools. (bgp|layer2)
metallb_ap1_protocol: 'bgp'
metallb_ap2_protocol: 'bgp'
metallb_ap3_protocol: 'bgp'
metallb_ap4_protocol: 'bgp'

# The CIDRs of your address pools.
metallb_ap1_cidr: '192.168.10.1/24'
metallb_ap2_cidr: '192.168.50.1/24'
metallb_ap3_cidr: '192.168.60.1/24'
metallb_ap4_cidr: '192.168.70.1/24'


###############################################################################################
#                              -NGINX Ingress Controller Options-                             # 
###############################################################################################
# These parameters are only required if you want to enable the NGINX ingress controller.      #
# REQUIREMENT: The NGINX Ingress controller integration requires a Load Balancer integration. #
###############################################################################################

# The IP Address to use 
nginx_load_balancer_ip: '192.168.0.100'


###############################################################################################
#                                      -DataDog Options-                                      # 
###############################################################################################
# These parameters are only required if you have an account with DataDog and would like to    #
# enable metrics collection for your cluster.                                                 #
###############################################################################################

# Your DataDog API Key
dd_api_key: 'APIKEY'

# Namespace to deploy DataDog to
dd_namespace: 'default'
